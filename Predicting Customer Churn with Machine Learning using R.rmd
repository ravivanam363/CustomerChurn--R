---
title: "Predicting Customer Churn with Machine Learning using R"
author: "Ravi Vanam"
date: "May 6, 2017"
output: html_document
---
##Data Exploration
. One of the common tasks in Business Analytics
is to understand customer behavior.

. In many industries it is more expensive to find a
new customer than to entice an existing one to
stay. This is usually known as "churn" analysis.

. The aim is to accurately identify the cohort who
is likely to leave early enough so that the
relationship can be saved.

. So the goal is to predict the propensity to churn
of a customer from the service provider through
the usage behavior using various supervised
machine learning algorithms.

. This data is taken from a telecommunications
company and involves customer data for a
collection of customers who either stayed with
the company or left within a certain period.

. There are 19 predictors, mostly numeric

. The outcome is contained in a column called
churn (yes/no).

. The training data has 3333 samples and the test
set contains 1667.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r}
library(ggplot2)
```


```{r}
library(C50)
data(churn)
## Two objects are loaded: churnTrain and churnTest
str(churnTrain)
table(churnTrain$churn)
any(is.na(churnTrain))
```


```{r}
numCol <- sapply(churnTrain, is.numeric)
correlatedData <- cor(churnTrain[,numCol])

library(corrplot)

corrplot(correlatedData, method = 'pie',type = c("lower"))
```


```{r}
ggplot(data = churnTrain, aes(x = international_plan))+
  geom_bar(aes(fill = churn), position = 'dodge',alpha = 0.8)
#international plan vs churn
ggplot(churnTrain,aes(international_plan)) +
  geom_bar(aes(fill = factor(churn)),alpha = 0.8,position = "fill")


```

```{r}
library(caret)
ggplot(churnTrain,aes(churn))+ 
  geom_bar()
```
```{r}
#state vs churn
ggplot(churnTrain,aes(state)) + 
  geom_bar(aes(fill = factor(churn)),alpha = 0.8,position = "fill")
#state vs churn
ggplot(churnTrain,aes(state)) + 
  geom_bar(aes(fill = factor(churn)),alpha = 0.8,position = "dodge")
```

```{r}
library(gmodels)
s <- CrossTable(churnTrain$state, churnTrain$churn,format=c("SAS"),prop.chisq=TRUE)
sdf <- as.data.frame(s$prop.row)
sdf2y <- sdf[1:51,-2]
sdf3n <- sdf[52:102,-2]
state_freq <- merge(sdf2y,sdf3n, by='x')

library(reshape2)
sdf.long<-melt(sdf)
ggplot(sdf, aes(x = y,y=Freq,fill=x))+
  geom_bar(position="dodge",stat="identity")
```

```{r}
#voicemail plan vs churn
ggplot(churnTrain,aes(voice_mail_plan)) + 
  geom_bar(aes(fill = factor(churn)),alpha = 0.8, position = "fill")
#voicemail plan vs churn
ggplot(churnTrain,aes(voice_mail_plan)) + 
  geom_bar(aes(fill = factor(churn)),alpha = 0.8, position = "dodge")
```

```{r}
#number_voicemail_message vs churn
ggplot(churnTrain, aes(x= number_vmail_messages)) + 
  geom_bar(aes(fill = factor(churn)),alpha = 0.8, position = "fill")
#number_voicemail_message vs churn
ggplot(churnTrain, aes(x= number_vmail_messages)) +
  geom_bar(aes(fill = factor(churn)),alpha = 0.8, position = "dodge")

```

```{r}
#total_daycharge vs churn
ggplot(churnTrain, aes(x= total_day_charge)) +
  geom_histogram(aes(fill = factor(churn)),position = "fill")
#total_daycharge vs churn
ggplot(churnTrain, aes(x= total_day_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "dodge")
```

```{r}
#total_eveningcharge vs churn
ggplot(churnTrain, aes(x= total_eve_charge)) +
  geom_histogram(aes(fill = factor(churn)),position = "fill")
#total_eveningcharge vs churn
ggplot(churnTrain, aes(x= total_eve_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "dodge")
```

```{r}
#total_nightcharge vs churn
ggplot(churnTrain, aes(x= total_night_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "fill")
#total_nightcharge vs churn
ggplot(churnTrain, aes(x= total_night_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "dodge")
```

```{r}
#total_intlcharge vs churn
ggplot(churnTrain, aes(x= total_intl_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "fill")
#total_intlcharge vs churn
ggplot(churnTrain, aes(x= total_intl_charge)) + 
  geom_histogram(aes(fill = factor(churn)),position = "dodge")
```


```{r}

#number of customer service calls vs churn
ggplot(churnTrain, aes(x= number_customer_service_calls)) + 
  geom_bar(aes(fill = factor(churn)),position = "fill")
#number of customer service calls vs churn
ggplot(churnTrain, aes(x= number_customer_service_calls)) + 
  geom_bar(aes(fill = factor(churn)),position = "dodge")


```






removing correlated columns total_day_minutes,total_eve_minutes,total_night_minutes,total_intl_minutes 


```{r}
churnData <- rbind(churnTrain,churnTest)
churnData <- churnData[,-c(7,10,13,16)]

library(caret)
dataPartition <- createDataPartition(churnData$churn, p = 0.6664, list = FALSE)
trainData <- churnData[dataPartition, ]
testData <- churnData[-dataPartition,]
```


```{r}
logModel <- glm(formula = churn ~ . , family = binomial(link = 'logit'),data = trainData[,-1])
fitProbs <- predict(logModel,newdata=testData[,-16],type='response')
fitResults <- ifelse(fitProbs > 0.5,1,0)
churn_train1 <- as.factor(testData$churn)
churn_train2 <- ifelse(churn_train1 == "no",1,0)
misClasificError <- mean(fitResults != churn_train2)
print(paste('Accuracy',1-misClasificError))


```

#Two Hot Encoding
```{r}
testData2 <- testData
testData2$y <- ifelse(testData2$churn == "no",1,0)
testData2$yProb <- fitProbs
TwohotData <- testData2[,c(1,17,18)]
TwohotData$Diff <- TwohotData$y - TwohotData$yProb
twoHotEncoding <- aggregate(TwohotData[,4], list(TwohotData$state), mean) #average of each state
twoHotEncoding
colnames(twoHotEncoding) <- c('state','stateValue')

library(dplyr)
testDataFinal <- testData %>% left_join(twoHotEncoding, by = "state")
testDataFinal <- as.data.frame(testDataFinal[,-1])

trainDataFinal <- trainData %>% left_join(twoHotEncoding, by = "state")
trainDataFinal <- trainDataFinal[,-1]
```



after encoding
```{r}

logModel2 <- glm(formula = churn ~ . , family = binomial(link = 'logit'),data = trainDataFinal)
predsLog <- predict(logModel2,newdata=testDataFinal[,-15],type='response')
fitResultsLog <- ifelse(predsLog > 0.5,1,0)
churn_t1 <- as.factor(testDataFinal$churn)
churn_t2 <- ifelse(churn_t1 == "no",1,0)
misClasificError <- mean(fitResultsLog != churn_t2)
print(paste('Accuracy',1-misClasificError))
summary(logModel2)
```

From the above logistic regression output, we can see the coefficients of various predictors.
The following variables are driving the customer churn the most.
	1. total_day_charge, 
	2. number_customer_service_calls, 
	3. total_eve_charge 
	4. international_planyes, 
	5. voice_mail_planyes ,
	6. total_intl_calls 
It means if a customer subscribes to voice mail, he/she may more likely to churn.
Similarly if a customer subscribes to international plan, he/she may more likely to churn. 
	To reduce churn, the company can
1. Reduce the cost of voice_mail_plan
2. Reduce the call cost of international calls
3. Reduce charges levied for the services provided and
4. Provide better customer service and reduce the number of service calls made by a customer
 The Accuracy of our model = 86.92%



```{r}
library(car)
#influenceIndexPlot(logModel2)  #cook's d

exp(coef(logModel2))

exp(cbind(OR = coef(logModel2), confint(logModel2)))

```



```{r}
testDataFinal2 <- testDataFinal
testDataFinal2$churn <- ifelse(as.factor(testDataFinal$churn) == "no",1,0)
trainDataFinal2 <- trainDataFinal
trainDataFinal2$churn <- ifelse(as.factor(trainDataFinal$churn) == "no",1,0)

library(C50)
trainDataFinal2$churn <- as.factor(trainDataFinal2$churn)
c50model <- C5.0(churn ~ . , data = trainDataFinal2, rules = TRUE)
summary(c50model)
C5imp(c50model, metric = 'usage')
C5imp(c50model, metric = 'splits')
```


```{r}
predsC50 <- predict(c50model,newdata = testDataFinal2[,-15],type='prob')
fitResultsC50 <- ifelse(predsC50 > 0.6,1,0) #.7 95.02,.8 92.26,0.6 95.02,.5 95.02

confusionMatrix(fitResultsC50[,2],testDataFinal2$churn)
```

for different cut-offs their accuracies are
Cut-off = .8, accuracy = 90.64%, 
 .7 93.28%,
 .6 94.84%,
 .5 95.26%,
 .75 92.14%,
 0.9 81.16%,
 .85 87.64%
```{r}

library(randomForest)
rf2 <- randomForest(churn ~ . ,
                   data = trainDataFinal2,
                   ntree = 500,
                   mtry = 4,
                   replace = TRUE,
                   nodesize = 10,
                   do.trace = 10)
print(rf2)
importance(rf2)
varImpPlot(rf2)
predsRf <- predict(rf2,newdata=testDataFinal2[,-15],type='prob')
fitResultsRf2 <- ifelse(predsRf > 0.5,1,0) 

confusionMatrix(fitResultsRf2[,2],testDataFinal2$churn)
```


```{r}
library(rpart)
treeModel <- rpart(churn ~ ., method = 'class',data = trainDataFinal2)
predsTree <- predict(treeModel,newdata = testDataFinal2[,-15],type='prob')
fitResultsTree <- ifelse(predsTree > 0.6,1,0) 

confusionMatrix(fitResultsTree[,2],testDataFinal2$churn)
```


```{r}
library(ROCR)
preds3 <- prediction(fitResultsC50[,2], testDataFinal2$churn)
preds2 <- prediction(fitResultsRf2[,2], testDataFinal2$churn)
preds1 <- prediction(fitResultsTree[,2], testDataFinal2$churn)

perf1 <- performance(preds1, "tpr", "fpr")
perf2 <- performance(preds2, "tpr", "fpr")
perf3 <- performance(preds3, "tpr", "fpr")

plot.new()
plot(perf1, col = 'orange', lwd = 2.5)
plot(perf2, add = TRUE, col = 'green',lwd = 2.5)
plot(perf3, add = TRUE, col = 'blue',lwd = 2.5)
abline(0,1, col = "red", lwd = 2.5, lty = 2)

title('ROC Curve')
legend(0.8,0.4, c('Tree','RF','C50'),
  lty = c(1,1,1),
  lwd=c(1.4,1.4,1.4),col = c('orange','green','blue','yellow'))
```

AUC Calculation Metrics

```{r}
fitAuc1 <- performance(preds1,'auc')
fitAuc2 <- performance(preds2,'auc')
fitAuc3 <- performance(preds3,'auc')
fitAuc1
fitAuc2
fitAuc3

```

Best model is Random forest, for different cut-offs their accuracies are
Cut-off = .8, accuracy = 90.64%, 
.7 93.28, 
.6 94.84, 
.5 95.26, 
.75 92.14,
0.9 81.16,
.85 87.64
```{r}
fitResultsRf3 <- ifelse(predsRf > 0.5,1,0) 
confMtrx1 <- confusionMatrix(fitResultsRf3[,2],testDataFinal2$churn)

fitResultsRf4 <- ifelse(predsRf > 0.6,1,0) 
confMtrx2 <- confusionMatrix(fitResultsRf4[,2],testDataFinal2$churn)

fitResultsRf4 <- ifelse(predsRf > 0.7,1,0) 
confMtrx3 <- confusionMatrix(fitResultsRf4[,2],testDataFinal2$churn)

confMtrx1
confMtrx2
confMtrx3
confMtrx3$table
```

         
         
